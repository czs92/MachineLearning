{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultinomialNaiveBayes():\n",
    "    def __init__(self):\n",
    "        self.prob_vec = {}\n",
    "        self.classes  = []\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        # extract the probabilities for each element for different classes\n",
    "        self.classes = np.unique(y)\n",
    "        occurences = np.where(X != 0, 1.0, 0.0)\n",
    "        for c in self.classes:\n",
    "            idx = np.where(y==c)[0]\n",
    "            N = len(idx)\n",
    "            \n",
    "            self.prob_vec[c] = np.log(np.sum(occurences[idx,:],axis=0)/N+1e-10)\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        #Output matrix N_documents X N_classes\n",
    "        probs = np.zeros((X.shape[0],len(self.classes)))\n",
    "        i=0\n",
    "        for k in self.classes:\n",
    "\n",
    "            probs[:,i] = np.dot(X,self.prob_vec[k].T).ravel()\n",
    "            i+=1\n",
    "        return probs\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.classes[np.argmax(self.predict_proba(X),axis =1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=np.array([[1,2,3,0,0,0],[0,1,2,0,0,1],[0,0,0,2,3,1],[1,0,1,2,3,0]],dtype=np.float)\n",
    "y = [0,0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = BernoulliNaiveBayes()\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([ -6.93147180e-01,   1.00000008e-10,   1.00000008e-10,\n",
       "         -2.30258509e+01,  -2.30258509e+01,  -6.93147180e-01]),\n",
       " 1: array([ -6.93147180e-01,  -2.30258509e+01,  -6.93147180e-01,\n",
       "          1.00000008e-10,   1.00000008e-10,  -6.93147180e-01])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.prob_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -0.69314718,  -48.82429058],\n",
       "       [  -0.69314718,  -25.10529247],\n",
       "       [-115.82240183,   -0.69314718],\n",
       "       [-115.82240183,   -1.38629436]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NY Times dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "import pandas as pd\n",
    "data=pd.read_csv('./files/Boydstun_NYT_FrontPage_Dataset_1996-2006_0.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Let us train the classifier with data up to 1/1/2004 and test its performnace in data from 2004-2006\n",
    "split = pd.to_datetime(pd.Series(data['Date']))<pd.datetime(2004, 1, 1)\n",
    "raw_data = data['Title']\n",
    "raw_train = raw_data[split]\n",
    "raw_test = raw_data[np.logical_not(split)]\n",
    "y = data['Topic_2digit']\n",
    "y_train = y[split]\n",
    "y_test = y[np.logical_not(split)]\n",
    "print ('Check the split sizes, train, test and total amount of data:')\n",
    "print (raw_train.shape, raw_test.shape, raw_data.shape)\n",
    "print ('Display the labels:')\n",
    "print (np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us tokenize the data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# We use the count number of instances considering that a word has a minimum support of two documents\n",
    "vectorizer = CountVectorizer(min_df=2, \n",
    "# stop words such as 'and', 'the', 'of' are removed                             \n",
    " stop_words='english', \n",
    " strip_accents='unicode')\n",
    "\n",
    "#example of the tokenization\n",
    "test_string = raw_train[0]\n",
    "print (\"Example: \" + test_string +\"\\n\")\n",
    "print (\"Preprocessed: \" + vectorizer.build_preprocessor()(test_string)+\"\\n\")\n",
    "print (\"Tokenized:\" + str(vectorizer.build_tokenizer()(test_string))+\"\\n\")\n",
    "print (\"Analyzed data string:\" + str(vectorizer.build_analyzer()(test_string))+\"\\n\")\n",
    "\n",
    "\n",
    "#Process and convert data\n",
    "X_train = vectorizer.fit_transform(raw_train)\n",
    "X_test = vectorizer.transform(raw_test)\n",
    "\n",
    "print (\"Number of tokens: \" + str(len(vectorizer.get_feature_names())) +\"\\n\")\n",
    "print (\"Extract of tokens:\")\n",
    "print (vectorizer.get_feature_names()[1000:1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.todense()\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.todense()\n",
    "X_test = X_test.astype(np.float)\n",
    "y_train = np.array(y_train.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Multinomial NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNaiveBayes()\n",
    "clf.fit(X_train,y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(y_pred, y):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest',cmap='gray')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('true value')\n",
    "    plt.xlabel('predicted value')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9,9)    \n",
    "    \n",
    "print (\"classification accuracy:\", metrics.accuracy_score(y_hat, y_test))\n",
    "plot_confusion_matrix(y_hat, y_test)\n",
    "print (\"Classification Report:\")\n",
    "print (metrics.classification_report(y_hat,np.array(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result of sklearn Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train,y_train)\n",
    "y_hat = nb.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(y_pred, y):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest',cmap='gray')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('true value')\n",
    "    plt.xlabel('predicted value')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9,9)    \n",
    "    \n",
    "print (\"classification accuracy:\", metrics.accuracy_score(y_hat, y_test))\n",
    "plot_confusion_matrix(y_hat, y_test)\n",
    "print (\"Classification Report:\")\n",
    "print (metrics.classification_report(y_hat,np.array(y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
